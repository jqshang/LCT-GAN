{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8212e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\LCT-GAN\\.venv\\Lib\\site-packages\\df\\io.py:9: UserWarning: `torchaudio.backend.common.AudioMetaData` has been moved to `torchaudio.AudioMetaData`. Please update the import path.\n",
      "  from torchaudio.backend.common import AudioMetaData\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "import soundfile as sf\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().resolve().parent))\n",
    "from models.generator import LCTGenerator, LCTGeneratorConfig\n",
    "from util import resample_tensor, ModelComparator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee2501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit = torch.jit.load(\"D:/Projects/LCT-GAN/.data/FTFNet_scripted.pt\", map_location=device)\n",
    "ftf = jit.model.eval()\n",
    "\n",
    "my_gen = LCTGenerator(LCTGeneratorConfig()).to(device).eval()\n",
    "my_gen.load_state_dict(ftf.state_dict(), strict=True)\n",
    "\n",
    "cmp = ModelComparator(lct=jit, my_lct=my_gen, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b39b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_path = \"D:/Projects/LCT-GAN/.data/subjective_test_audios/impulse/noisy_fileid_1_snr14.25_tl-23.wav\"\n",
    "out_dir = Path(\"D:/Projects/LCT-GAN/.data/match_test/impulse\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c08ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max |FTFNet_wav - my_wav|: 0.5958443880081177\n",
      "mean |FTFNet_wav - my_wav|: 0.04623037949204445\n"
     ]
    }
   ],
   "source": [
    "x_np, sr_in = sf.read(noisy_path, dtype=\"float32\")\n",
    "if x_np.ndim > 1:\n",
    "    x_np = x_np.mean(axis=1)\n",
    "\n",
    "x = torch.from_numpy(x_np).unsqueeze(0).to(device)\n",
    "noisy_16k = resample_tensor(x, sr_in, 16000)\n",
    "noisy_16k = torch.clamp(noisy_16k, -1.0, 1.0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_ftf = cmp.run_lct_gan(noisy_16k)\n",
    "    y_my  = cmp.run_my_lct_gan(noisy_16k)\n",
    "\n",
    "y_ftf = torch.clamp(y_ftf, -1.0, 1.0)\n",
    "y_my  = torch.clamp(y_my,  -1.0, 1.0)\n",
    "\n",
    "T = min(y_ftf.shape[-1], y_my.shape[-1])\n",
    "diff = (y_ftf[..., :T] - y_my[..., :T]).abs()\n",
    "\n",
    "print(\"max |FTFNet_wav - my_wav|:\", diff.max().item())\n",
    "print(\"mean |FTFNet_wav - my_wav|:\", diff.mean().item())\n",
    "\n",
    "sf.write(str(out_dir / \"noisy_16k.wav\"), noisy_16k.squeeze(0).detach().cpu().numpy(), 16000)\n",
    "sf.write(str(out_dir / \"ftfnet_16k.wav\"), y_ftf.squeeze(0).detach().cpu().numpy(), 16000)\n",
    "sf.write(str(out_dir / \"my_ftfnet_16k.wav\"), y_my.squeeze(0).detach().cpu().numpy(), 16000)\n",
    "sf.write(str(out_dir / \"diff_ftf_minus_my.wav\"),\n",
    "         (y_ftf[..., :T] - y_my[..., :T]).squeeze(0).detach().cpu().numpy(),\n",
    "         16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fb5228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\LCT-GAN\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max |mask_ftf_c - mask_my_c|: 2.0696914196014404\n",
      "mean |mask_ftf_c - mask_my_c|: 0.39401867985725403\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().resolve().parent))\n",
    "from models.generator import LCTGenerator, LCTGeneratorConfig\n",
    "from datasets.stft import make_lct_stft, magnitude, apply_mask\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "noisy_wav_path = r\"D:/Projects/LCT-GAN/.data/subjective_test_audios/impulse/noisy_fileid_1_snr14.25_tl-23.wav\"\n",
    "jit_path = r\"D:/Projects/LCT-GAN/.data/FTFNet_scripted.pt\"\n",
    "\n",
    "# audio\n",
    "wav, sr = torchaudio.load(noisy_wav_path)\n",
    "wav = wav.mean(dim=0, keepdim=True)\n",
    "if sr != 16000:\n",
    "    wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "wav = wav.to(device)                 # [1, N]\n",
    "noisy = wav.unsqueeze(0)             # [B=1, 1, N]\n",
    "noisy_bt = noisy.squeeze(1)          # [B, N]\n",
    "\n",
    "# models\n",
    "jit = torch.jit.load(jit_path, map_location=device)\n",
    "ftf = jit.model.eval()\n",
    "\n",
    "my_gen = LCTGenerator(LCTGeneratorConfig()).to(device).eval()\n",
    "my_gen.load_state_dict(ftf.state_dict(), strict=True)\n",
    "\n",
    "# STFT (project canonical)\n",
    "stft = make_lct_stft(n_fft=512)\n",
    "noisy_stft = stft(noisy_bt)                          # [B, F, T]\n",
    "noisy_mag = magnitude(noisy_stft).unsqueeze(1)       # [B, 1, F, T]\n",
    "\n",
    "# noisy_mag: [B, 1, F, T]\n",
    "with torch.no_grad():\n",
    "    mask_my_c = my_gen(noisy_mag)  # [B, 1, F, T]\n",
    "\n",
    "    mask_ftf_c_TF = ftf(noisy_mag.permute(0, 1, 3, 2).contiguous())  # [B, 1, T, F] (typical)\n",
    "    mask_ftf_c = mask_ftf_c_TF.permute(0, 1, 3, 2).contiguous()      # [B, 1, F, T]\n",
    "\n",
    "    diff = (mask_ftf_c - mask_my_c).abs()\n",
    "    print(\"max |mask_ftf_c - mask_my_c|:\", diff.max().item())\n",
    "    print(\"mean |mask_ftf_c - mask_my_c|:\", diff.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8acb801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTF conv1: (2, 3) (1, 2) (0, 0)\n",
      "MY  conv1: (2, 3) (1, 2) (0, 0)\n",
      "FTF conv2: (2, 3) (1, 2) (0, 0)\n",
      "MY  conv2: (2, 3) (1, 2) (0, 0)\n",
      "FTF conv3: (2, 3) (1, 2) (0, 0)\n",
      "MY  conv3: (2, 3) (1, 2) (0, 0)\n",
      "FTF deconv2: (2, 3) (1, 2) (0, 0) (0, 0)\n",
      "MY  deconv2: (2, 3) (1, 2) (0, 0) (0, 0)\n",
      "FTF deconv3: (2, 3) (1, 2) (0, 0) (0, 1)\n",
      "MY  deconv3: (2, 3) (1, 2) (0, 0) (0, 1)\n",
      "FTF deconv4: (2, 3) (1, 2) (0, 0) (0, 0)\n",
      "MY  deconv4: (2, 3) (1, 2) (0, 0) (0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(\"FTF conv1:\", ftf.conv1.kernel_size, ftf.conv1.stride, ftf.conv1.padding)\n",
    "print(\"MY  conv1:\", my_gen.conv1.kernel_size, my_gen.conv1.stride, my_gen.conv1.padding)\n",
    "\n",
    "print(\"FTF conv2:\", ftf.conv2.kernel_size, ftf.conv2.stride, ftf.conv2.padding)\n",
    "print(\"MY  conv2:\", my_gen.conv2.kernel_size, my_gen.conv2.stride, my_gen.conv2.padding)\n",
    "\n",
    "print(\"FTF conv3:\", ftf.conv3.kernel_size, ftf.conv3.stride, ftf.conv3.padding)\n",
    "print(\"MY  conv3:\", my_gen.conv3.kernel_size, my_gen.conv3.stride, my_gen.conv3.padding)\n",
    "\n",
    "print(\"FTF deconv2:\", ftf.deconv2.kernel_size, ftf.deconv2.stride, ftf.deconv2.padding, ftf.deconv2.output_padding)\n",
    "print(\"MY  deconv2:\", my_gen.deconv2.kernel_size, my_gen.deconv2.stride, my_gen.deconv2.padding, my_gen.deconv2.output_padding)\n",
    "\n",
    "print(\"FTF deconv3:\", ftf.deconv3.kernel_size, ftf.deconv3.stride, ftf.deconv3.padding, ftf.deconv3.output_padding)\n",
    "print(\"MY  deconv3:\", my_gen.deconv3.kernel_size, my_gen.deconv3.stride, my_gen.deconv3.padding, my_gen.deconv3.output_padding)\n",
    "\n",
    "print(\"FTF deconv4:\", ftf.deconv4.kernel_size, ftf.deconv4.stride, ftf.deconv4.padding, ftf.deconv4.output_padding)\n",
    "print(\"MY  deconv4:\", my_gen.deconv4.kernel_size, my_gen.deconv4.stride, my_gen.deconv4.padding, my_gen.deconv4.output_padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b8040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftf.activation: RecursiveScriptModule(original_name=LeakyReLU)\n"
     ]
    }
   ],
   "source": [
    "print(\"ftf.activation:\", ftf.activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e7a22",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "register_forward_hook is not supported on ScriptModules",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m store_my  = {}\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Choose comparable points\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mftf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_forward_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrab\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconv1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_ftf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m my_gen.conv1.register_forward_hook(grab(\u001b[33m\"\u001b[39m\u001b[33mconv1\u001b[39m\u001b[33m\"\u001b[39m, store_my))\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Also grab post-activation tensors by hooking conv2/conv3 etc. if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\LCT-GAN\\.venv\\Lib\\site-packages\\torch\\jit\\_script.py:987\u001b[39m, in \u001b[36m_make_fail.<locals>.fail\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfail\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m987\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(name + \u001b[33m\"\u001b[39m\u001b[33m is not supported on ScriptModules\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: register_forward_hook is not supported on ScriptModules"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def grab(name, store):\n",
    "    def _hook(m, inp, out):\n",
    "        store[name] = out.detach()\n",
    "    return _hook\n",
    "\n",
    "store_ftf = {}\n",
    "store_my  = {}\n",
    "\n",
    "# Choose comparable points\n",
    "ftf.conv1.register_forward_hook(grab(\"conv1\", store_ftf))\n",
    "my_gen.conv1.register_forward_hook(grab(\"conv1\", store_my))\n",
    "\n",
    "# Also grab post-activation tensors by hooking conv2/conv3 etc. if needed\n",
    "ftf.conv2.register_forward_hook(grab(\"conv2\", store_ftf))\n",
    "my_gen.conv2.register_forward_hook(grab(\"conv2\", store_my))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Pick the same underlying TF input to conv1 in both models:\n",
    "    x_FT = noisy_mag                       # [B,1,F,T] -> my_gen permutes internally\n",
    "    x_TF = noisy_mag.permute(0,1,3,2).contiguous()  # [B,1,T,F] -> ftf public interface (typical)\n",
    "\n",
    "    _ = my_gen(x_FT)\n",
    "    _ = ftf(x_TF)\n",
    "\n",
    "for k in [\"conv1\", \"conv2\"]:\n",
    "    a = store_my[k]\n",
    "    b = store_ftf[k]\n",
    "    # conv outputs should match extremely closely if inputs/padding/weights match\n",
    "    d = (a - b).abs()\n",
    "    print(k, \"max\", d.max().item(), \"mean\", d.mean().item(), \"shape\", tuple(a.shape), tuple(b.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0fe0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=FTFNet\n",
      "  (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (conv3): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (deconv2): RecursiveScriptModule(original_name=ConvTranspose2d)\n",
      "  (deconv3): RecursiveScriptModule(original_name=ConvTranspose2d)\n",
      "  (deconv4): RecursiveScriptModule(original_name=ConvTranspose2d)\n",
      "  (activation): RecursiveScriptModule(original_name=LeakyReLU)\n",
      "  (activations): RecursiveScriptModule(original_name=Sigmoid)\n",
      "  (skip2): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (skip3): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (skip4): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (GRUf1): RecursiveScriptModule(\n",
      "    original_name=GRUblockf\n",
      "    (gru1): RecursiveScriptModule(original_name=GRU)\n",
      "    (gru2): RecursiveScriptModule(original_name=GRU)\n",
      "    (gru3): RecursiveScriptModule(original_name=GRU)\n",
      "    (gru4): RecursiveScriptModule(original_name=GRU)\n",
      "    (attn): RecursiveScriptModule(\n",
      "      original_name=MultiheadAttention\n",
      "      (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
      "    )\n",
      "    (activationtrans): RecursiveScriptModule(original_name=LeakyReLU)\n",
      "    (layernorm1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "    (layernorm2): RecursiveScriptModule(original_name=LayerNorm)\n",
      "    (lin): RecursiveScriptModule(original_name=Linear)\n",
      "  )\n",
      "  (GRUf2): RecursiveScriptModule(\n",
      "    original_name=GRUblockf\n",
      "    (gru1): RecursiveScriptModule(original_name=GRU)\n",
      "    (gru2): RecursiveScriptModule(original_name=GRU)\n",
      "    (gru3): RecursiveScriptModule(original_name=GRU)\n",
      "    (gru4): RecursiveScriptModule(original_name=GRU)\n",
      "    (attn): RecursiveScriptModule(\n",
      "      original_name=MultiheadAttention\n",
      "      (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
      "    )\n",
      "    (activationtrans): RecursiveScriptModule(original_name=LeakyReLU)\n",
      "    (layernorm1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "    (layernorm2): RecursiveScriptModule(original_name=LayerNorm)\n",
      "    (lin): RecursiveScriptModule(original_name=Linear)\n",
      "  )\n",
      "  (GRUt1): RecursiveScriptModule(\n",
      "    original_name=GRUblockt\n",
      "    (gru1): RecursiveScriptModule(original_name=GRU)\n",
      "    (gru2): RecursiveScriptModule(original_name=GRU)\n",
      "    (gru3): RecursiveScriptModule(original_name=GRU)\n",
      "    (gru4): RecursiveScriptModule(original_name=GRU)\n",
      "    (attn): RecursiveScriptModule(\n",
      "      original_name=MultiheadAttention\n",
      "      (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
      "    )\n",
      "    (activationtrans): RecursiveScriptModule(original_name=LeakyReLU)\n",
      "    (layernorm1): RecursiveScriptModule(original_name=LayerNorm)\n",
      "    (layernorm2): RecursiveScriptModule(original_name=LayerNorm)\n",
      "    (lin): RecursiveScriptModule(original_name=Linear)\n",
      "  )\n",
      "  (layernorm): RecursiveScriptModule(original_name=LayerNorm)\n",
      "  (pad): RecursiveScriptModule(original_name=ConstantPad2d)\n",
      "  (act_final): RecursiveScriptModule(original_name=ReLU)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(ftf)          # see if it has act_final / activations modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49fcc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation slope: 0.03\n",
      "GRUf1 activationtrans slope: 0.03\n",
      "GRUt1 activationtrans slope: 0.03\n",
      "layernorm eps: 1e-05\n",
      "conv1: (2, 3) (1, 2) (0, 0)\n",
      "deconv3: (2, 3) (1, 2) (0, 0) (0, 1)\n",
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  b, c, t, f, = torch.size(x)\n",
      "  _0 = torch.contiguous(torch.permute(x, [0, 3, 2, 1]))\n",
      "  x22 = torch.view(_0, [torch.mul(b, f), t, c])\n",
      "  feat_size = self.feat_size\n",
      "  gru1 = self.gru1\n",
      "  _1 = torch.slice(torch.slice(torch.slice(x22), 1), 2, None, torch.floordiv(feat_size, 4))\n",
      "  gru_out1, _2, = (gru1).forward__0(_1, None, )\n",
      "  gru2 = self.gru2\n",
      "  _3 = torch.slice(torch.slice(x22), 1)\n",
      "  _4 = torch.floordiv(feat_size, 4)\n",
      "  _5 = torch.mul(torch.floordiv(feat_size, 4), 2)\n",
      "  _6 = (gru2).forward__0(torch.slice(_3, 2, _4, _5), None, )\n",
      "  gru_out2, _7, = _6\n",
      "  gru3 = self.gru3\n",
      "  _8 = torch.slice(torch.slice(x22), 1)\n",
      "  _9 = torch.mul(torch.floordiv(feat_size, 4), 2)\n",
      "  _10 = torch.mul(torch.floordiv(feat_size, 4), 3)\n",
      "  _11 = (gru3).forward__0(torch.slice(_8, 2, _9, _10), None, )\n",
      "  gru_out3, _12, = _11\n",
      "  gru4 = self.gru4\n",
      "  _13 = torch.slice(torch.slice(x22), 1)\n",
      "  _14 = torch.mul(torch.floordiv(feat_size, 4), 3)\n",
      "  _15 = torch.mul(torch.floordiv(feat_size, 4), 4)\n",
      "  _16 = (gru4).forward__0(torch.slice(_13, 2, _14, _15), None, )\n",
      "  gru_out4, _17, = _16\n",
      "  _18 = [gru_out1, gru_out2, gru_out3, gru_out4]\n",
      "  x_temp = torch.cat(_18, 2)\n",
      "  lin = self.lin\n",
      "  activationtrans = self.activationtrans\n",
      "  x_temp2 = (lin).forward((activationtrans).forward(x_temp, ), )\n",
      "  x23 = torch.add(x22, x_temp2)\n",
      "  layernorm2 = self.layernorm2\n",
      "  x24 = (layernorm2).forward(x23, )\n",
      "  device = self.device\n",
      "  _19 = torch.full([t, t], 1., dtype=None, layout=None, device=device)\n",
      "  _20 = torch.triu(_19, -64)\n",
      "  device0 = self.device\n",
      "  _21 = torch.full([t, t], 1., dtype=None, layout=None, device=device0)\n",
      "  mask = torch.sub(_20, torch.triu(_21, 1))\n",
      "  device1 = self.device\n",
      "  _22 = torch.full([t, t], 1., dtype=None, layout=None, device=device1)\n",
      "  mask0 = torch.mul(torch.sub(mask, _22), 1000000000.)\n",
      "  attn = self.attn\n",
      "  _23 = (attn).forward(x24, x24, x24, None, True, mask0, True, True, )\n",
      "  x_temp3, _24, = _23\n",
      "  x25 = torch.add(x24, x_temp3)\n",
      "  layernorm1 = self.layernorm1\n",
      "  x26 = (layernorm1).forward(x25, )\n",
      "  x27 = torch.permute(torch.view(x26, [b, f, t, c]), [0, 3, 2, 1])\n",
      "  return x27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"activation slope:\", ftf.activation.negative_slope)\n",
    "print(\"GRUf1 activationtrans slope:\", ftf.GRUf1.activationtrans.negative_slope)\n",
    "print(\"GRUt1 activationtrans slope:\", ftf.GRUt1.activationtrans.negative_slope)\n",
    "\n",
    "print(\"layernorm eps:\", ftf.layernorm.eps)\n",
    "\n",
    "print(\"conv1:\", ftf.conv1.kernel_size, ftf.conv1.stride, ftf.conv1.padding)\n",
    "print(\"deconv3:\", ftf.deconv3.kernel_size, ftf.deconv3.stride, ftf.deconv3.padding, ftf.deconv3.output_padding)\n",
    "\n",
    "# To detect causal/trapezoid masking in scripted code:\n",
    "print(ftf.GRUt1.code)      # or: print(ftf.GRUt1.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71dce06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.lct_la1n.FTFNet,\n",
      "      %x.1 : Tensor):\n",
      "  %114 : NoneType = prim::Constant() # :0:0\n",
      "  %51 : int = prim::Constant[value=0]() # /project_ghent/stress/lct_la1n.py:141:22\n",
      "  %52 : int = prim::Constant[value=2]() # /project_ghent/stress/lct_la1n.py:141:25\n",
      "  %53 : int = prim::Constant[value=3]() # /project_ghent/stress/lct_la1n.py:141:28\n",
      "  %54 : int = prim::Constant[value=1]() # /project_ghent/stress/lct_la1n.py:141:31\n",
      "  %100 : int = prim::Constant[value=-1]() # /project_ghent/stress/lct_la1n.py:150:34\n",
      "  %121 : int = prim::Constant[value=-2]() # /project_ghent/stress/lct_la1n.py:154:34\n",
      "  %142 : int = prim::Constant[value=-3]() # /project_ghent/stress/lct_la1n.py:158:34\n",
      "  %pad.1 : __torch__.torch.nn.modules.padding.ConstantPad2d = prim::GetAttr[name=\"pad\"](%self.1)\n",
      "  %x0.1 : Tensor = prim::CallMethod[name=\"forward\"](%pad.1, %x.1) # /project_ghent/stress/lct_la1n.py:131:12\n",
      "  %7 : Tensor[] = prim::ListConstruct()\n",
      "  %activation.1 : __torch__.torch.nn.modules.activation.LeakyReLU = prim::GetAttr[name=\"activation\"](%self.1)\n",
      "  %conv1.1 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=\"conv1\"](%self.1)\n",
      "  %15 : Tensor = prim::CallMethod[name=\"forward\"](%conv1.1, %x0.1) # /project_ghent/stress/lct_la1n.py:133:28\n",
      "  %x1.1 : Tensor = prim::CallMethod[name=\"forward\"](%activation.1, %15) # /project_ghent/stress/lct_la1n.py:133:12\n",
      "  %19 : Tensor[] = aten::append(%7, %x1.1) # /project_ghent/stress/lct_la1n.py:134:8\n",
      "  %activation.5 : __torch__.torch.nn.modules.activation.LeakyReLU = prim::GetAttr[name=\"activation\"](%self.1)\n",
      "  %conv2.1 : __torch__.torch.nn.modules.conv.___torch_mangle_0.Conv2d = prim::GetAttr[name=\"conv2\"](%self.1)\n",
      "  %27 : Tensor = prim::CallMethod[name=\"forward\"](%conv2.1, %x1.1) # /project_ghent/stress/lct_la1n.py:135:28\n",
      "  %x2.1 : Tensor = prim::CallMethod[name=\"forward\"](%activation.5, %27) # /project_ghent/stress/lct_la1n.py:135:12\n",
      "  %31 : Tensor[] = aten::append(%7, %x2.1) # /project_ghent/stress/lct_la1n.py:136:8\n",
      "  %activation.7 : __torch__.torch.nn.modules.activation.LeakyReLU = prim::GetAttr[name=\"activation\"](%self.1)\n",
      "  %conv3.1 : __torch__.torch.nn.modules.conv.___torch_mangle_1.Conv2d = prim::GetAttr[name=\"conv3\"](%self.1)\n",
      "  %39 : Tensor = prim::CallMethod[name=\"forward\"](%conv3.1, %x2.1) # /project_ghent/stress/lct_la1n.py:137:28\n",
      "  %x3.1 : Tensor = prim::CallMethod[name=\"forward\"](%activation.7, %39) # /project_ghent/stress/lct_la1n.py:137:12\n",
      "  %43 : Tensor[] = aten::append(%7, %x3.1) # /project_ghent/stress/lct_la1n.py:138:8\n",
      "  %45 : int[] = aten::size(%x3.1) # /project_ghent/stress/lct_la1n.py:140:21\n",
      "  %b.1 : int, %c.1 : int, %t.1 : int, %f.1 : int = prim::ListUnpack(%45)\n",
      "  %55 : int[] = prim::ListConstruct(%51, %52, %53, %54)\n",
      "  %56 : Tensor = aten::permute(%x3.1, %55) # /project_ghent/stress/lct_la1n.py:141:12\n",
      "  %58 : Tensor = aten::contiguous(%56, %51) # /project_ghent/stress/lct_la1n.py:141:12\n",
      "  %62 : int = aten::mul(%b.1, %t.1) # /project_ghent/stress/lct_la1n.py:141:52\n",
      "  %65 : int[] = prim::ListConstruct(%62, %f.1, %c.1)\n",
      "  %x4.1 : Tensor = aten::view(%58, %65) # /project_ghent/stress/lct_la1n.py:141:12\n",
      "  %layernorm.1 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name=\"layernorm\"](%self.1)\n",
      "  %x5.1 : Tensor = prim::CallMethod[name=\"forward\"](%layernorm.1, %x4.1) # /project_ghent/stress/lct_la1n.py:142:12\n",
      "  %77 : int[] = prim::ListConstruct(%b.1, %t.1, %f.1, %c.1)\n",
      "  %78 : Tensor = aten::view(%x5.1, %77) # /project_ghent/stress/lct_la1n.py:143:12\n",
      "  %79 : int[] = prim::ListConstruct(%51, %53, %54, %52)\n",
      "  %x6.1 : Tensor = aten::permute(%78, %79) # /project_ghent/stress/lct_la1n.py:143:12\n",
      "  %GRUf1.1 : __torch__.lct_la1n.GRUblockf = prim::GetAttr[name=\"GRUf1\"](%self.1)\n",
      "  %x7.1 : Tensor = prim::CallMethod[name=\"forward\"](%GRUf1.1, %x6.1) # /project_ghent/stress/lct_la1n.py:146:12\n",
      "  %GRUt1.1 : __torch__.lct_la1n.GRUblockt = prim::GetAttr[name=\"GRUt1\"](%self.1)\n",
      "  %x8.1 : Tensor = prim::CallMethod[name=\"forward\"](%GRUt1.1, %x7.1) # /project_ghent/stress/lct_la1n.py:147:12\n",
      "  %GRUf2.1 : __torch__.lct_la1n.GRUblockf = prim::GetAttr[name=\"GRUf2\"](%self.1)\n",
      "  %x9.1 : Tensor = prim::CallMethod[name=\"forward\"](%GRUf2.1, %x8.1) # /project_ghent/stress/lct_la1n.py:148:12\n",
      "  %skip2.1 : __torch__.torch.nn.modules.conv.___torch_mangle_4.Conv2d = prim::GetAttr[name=\"skip2\"](%self.1)\n",
      "  %101 : Tensor = aten::__getitem__(%7, %100) # /project_ghent/stress/lct_la1n.py:150:26\n",
      "  %temp.1 : Tensor = prim::CallMethod[name=\"forward\"](%skip2.1, %101) # /project_ghent/stress/lct_la1n.py:150:15\n",
      "  %x10.1 : Tensor = aten::add(%x9.1, %temp.1, %54) # /project_ghent/stress/lct_la1n.py:151:12\n",
      "  %activation.9 : __torch__.torch.nn.modules.activation.LeakyReLU = prim::GetAttr[name=\"activation\"](%self.1)\n",
      "  %deconv2.1 : __torch__.torch.nn.modules.conv.ConvTranspose2d = prim::GetAttr[name=\"deconv2\"](%self.1)\n",
      "  %115 : Tensor = prim::CallMethod[name=\"forward\"](%deconv2.1, %x10.1, %114) # /project_ghent/stress/lct_la1n.py:152:28\n",
      "  %x11.1 : Tensor = prim::CallMethod[name=\"forward\"](%activation.9, %115) # /project_ghent/stress/lct_la1n.py:152:12\n",
      "  %skip3.1 : __torch__.torch.nn.modules.conv.___torch_mangle_5.Conv2d = prim::GetAttr[name=\"skip3\"](%self.1)\n",
      "  %122 : Tensor = aten::__getitem__(%7, %121) # /project_ghent/stress/lct_la1n.py:154:26\n",
      "  %temp0.1 : Tensor = prim::CallMethod[name=\"forward\"](%skip3.1, %122) # /project_ghent/stress/lct_la1n.py:154:15\n",
      "  %x12.1 : Tensor = aten::add(%x11.1, %temp0.1, %54) # /project_ghent/stress/lct_la1n.py:155:12\n",
      "  %activation : __torch__.torch.nn.modules.activation.LeakyReLU = prim::GetAttr[name=\"activation\"](%self.1)\n",
      "  %deconv3.1 : __torch__.torch.nn.modules.conv.___torch_mangle_2.ConvTranspose2d = prim::GetAttr[name=\"deconv3\"](%self.1)\n",
      "  %136 : Tensor = prim::CallMethod[name=\"forward\"](%deconv3.1, %x12.1, %114) # /project_ghent/stress/lct_la1n.py:156:28\n",
      "  %x13.1 : Tensor = prim::CallMethod[name=\"forward\"](%activation, %136) # /project_ghent/stress/lct_la1n.py:156:12\n",
      "  %skip4.1 : __torch__.torch.nn.modules.conv.___torch_mangle_6.Conv2d = prim::GetAttr[name=\"skip4\"](%self.1)\n",
      "  %143 : Tensor = aten::__getitem__(%7, %142) # /project_ghent/stress/lct_la1n.py:158:26\n",
      "  %temp1.1 : Tensor = prim::CallMethod[name=\"forward\"](%skip4.1, %143) # /project_ghent/stress/lct_la1n.py:158:15\n",
      "  %x14.1 : Tensor = aten::add(%x13.1, %temp1.1, %54) # /project_ghent/stress/lct_la1n.py:159:12\n",
      "  %act_final.1 : __torch__.torch.nn.modules.activation.ReLU = prim::GetAttr[name=\"act_final\"](%self.1)\n",
      "  %deconv4.1 : __torch__.torch.nn.modules.conv.___torch_mangle_3.ConvTranspose2d = prim::GetAttr[name=\"deconv4\"](%self.1)\n",
      "  %157 : Tensor = prim::CallMethod[name=\"forward\"](%deconv4.1, %x14.1, %114) # /project_ghent/stress/lct_la1n.py:161:27\n",
      "  %x15.1 : Tensor = prim::CallMethod[name=\"forward\"](%act_final.1, %157) # /project_ghent/stress/lct_la1n.py:161:12\n",
      "  %164 : Tensor = aten::slice(%x15.1, %51, %114, %114, %54) # /project_ghent/stress/lct_la1n.py:163:12\n",
      "  %168 : Tensor = aten::slice(%164, %54, %114, %114, %54) # /project_ghent/stress/lct_la1n.py:163:12\n",
      "  %171 : Tensor = aten::slice(%168, %52, %114, %121, %54) # /project_ghent/stress/lct_la1n.py:163:12\n",
      "  %176 : Tensor = aten::slice(%171, %53, %114, %114, %54) # /project_ghent/stress/lct_la1n.py:163:12\n",
      "  return (%176)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ftf.graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
